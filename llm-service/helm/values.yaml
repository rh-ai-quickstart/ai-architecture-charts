device: gpu  # Default device for models that don't specify one
rawDeploymentMode: true

# Device-specific configurations for tolerations and accelerators
deviceConfigs:
  gpu:
    tolerations:
      - key: nvidia.com/gpu
        effect: NoSchedule
        operator: Exists
    recommendedAccelerators:
      - nvidia.com/gpu
    acceleratorType: nvidia.com/gpu
  hpu:
    tolerations:
      - key: habana.ai/gaudi
        effect: NoSchedule
        operator: Exists
    recommendedAccelerators:
      - habana.ai/gaudi
    acceleratorType: habana.ai/gaudi
  cpu:
    tolerations: []
    recommendedAccelerators: []
    acceleratorType: null

servingRuntime:
  name: vllm-serving-runtime
  knativeTimeout: 60m
  gpuImage: quay.io/ecosystem-appeng/vllm:openai-v0.9.2
  cpuImage: quay.io/ecosystem-appeng/vllm:cpu-v0.9.2
  hpuImage: quay.io/modh/vllm:vllm-gaudi-v2-22-on-push-jgj5q-build-container
  env:
    - name: HOME
      value: /vllm
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          key: HF_TOKEN
          name: huggingface-secret
  volumeMounts:
    - name: shm
      mountPath: /dev/shm
    - name: vllm-chat-templates
      mountPath: /chat-templates
    - name: vllm-home
      mountPath: /vllm
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi
    - name: vllm-chat-templates
      configMap:
        name: vllm-chat-templates
    - name: vllm-home
      emptyDir: {}

secret:
  enabled: true
  hf_token: ""

models:
  llama-3-2-1b-instruct:
    id: meta-llama/Llama-3.2-1B-Instruct
    enabled: false
    device: hpu  # Can be gpu, hpu, or cpu - defaults to global device if not specified
    # A default "nvidia.com/gpu" toleration is implemented in the
    # inference-service.yaml template and can be overriden as follows:
    # tolerations:
    #   - key: nvidia.com/gpu
    #     effect: NoSchedule
    #     operator: Exists
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-guard-3-1b:
    id: meta-llama/Llama-Guard-3-1B
    enabled: false
    device: hpu
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-3-2-3b-instruct:
    id: meta-llama/Llama-3.2-3B-Instruct
    enabled: false
    device: hpu
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-guard-3-8b:
    id: meta-llama/Llama-Guard-3-8B
    enabled: false
    device: hpu
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-3-1-8b-instruct:
    id: meta-llama/Llama-3.1-8B-Instruct
    enabled: false
    device: hpu
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json

  llama-3-3-70b-instruct:
    id: meta-llama/Llama-3.3-70B-Instruct
    enabled: false
    device: hpu
    storageSize: 150Gi
    accelerators: "4"  # Number of accelerators needed different than 1
    resources:
      limits:
        cpu: "2"
        memory: 300Gi
      requests:
        cpu: "1"
        memory: 200Gi
    args:
      - --tensor-parallel-size
      - "4"
      - --gpu-memory-utilization
      - "0.95"
      - --max-num-batched-tokens
      - "131074"
      - --max-num-seqs
      - "32"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --swap-space
      - "32"

  llama-3-3-70b-instruct-quantization-fp8:
    id: meta-llama/Llama-3.3-70B-Instruct
    enabled: false
    device: gpu
    storageSize: 150Gi
    accelerators: "4"
    args:
      - --tensor-parallel-size
      - "4"
      - --gpu-memory-utilization
      - "0.95"
      - --quantization
      - fp8
      - --max-num-batched-tokens
      - "4096"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --swap-space
      - "32"

  llama-3-2-1b-instruct-quantized:
    id: RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8
    enabled: false
    device: gpu
    args:
      - --gpu-memory-utilization
      - "0.4"
      - --quantization
      - compressed-tensors
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"

  qwen-2-5-vl-3b-instruct:
    id: Qwen/Qwen2.5-VL-3B-Instruct
    device: gpu
    args:
      - --max-model-len
      - "30544"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_qwen.jinja
      - --tool-call-parser
      - llama3_json
